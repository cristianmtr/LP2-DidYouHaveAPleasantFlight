{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression,Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import random\n",
    "import os\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data needs to be in one file, one tweet per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"x-stemmer.csv\", index_col=0, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>570306133677760513</th>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570301130888122368</th>\n",
       "      <td>plu ad commerci experi tacki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570301083672813571</th>\n",
       "      <td>today must mean need take anoth trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570301031407624196</th>\n",
       "      <td>realli aggress blast obnoxi entertain guest fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570300817074462722</th>\n",
       "      <td>realli big bad thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    1\n",
       "0                                                                    \n",
       "570306133677760513                                               said\n",
       "570301130888122368                       plu ad commerci experi tacki\n",
       "570301083672813571               today must mean need take anoth trip\n",
       "570301031407624196  realli aggress blast obnoxi entertain guest fa...\n",
       "570300817074462722                               realli big bad thing"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data.txt', 'w', encoding='utf8')\n",
    "for i, row in X.iterrows():\n",
    "    f.write(\"%s\\n\" %row.ix[1])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  LabeledLineSentenceLabeled (object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffled = list(self.sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        return shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {'data.txt':'data'}\n",
    "sentences = LabeledLineSentenceLabeled(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "model.build_vocab(sentences.to_array())\n",
    "nr_words = sum([len(x.words) for x in sentences.sentences_perm()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1561435"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(sentences.sentences_perm(), epochs=20, total_words=nr_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('data-20.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('haha', 0.9998568892478943),\n",
       " ('pictur', 0.9998269081115723),\n",
       " ('winter', 0.9998247027397156),\n",
       " ('train', 0.999823808670044),\n",
       " ('tho', 0.9998233318328857),\n",
       " ('video', 0.9998232126235962),\n",
       " ('choic', 0.9998228549957275),\n",
       " ('sf', 0.9998192191123962),\n",
       " ('awesom', 0.9998174905776978),\n",
       " ('game', 0.9998162388801575)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get WE of sentence\n",
    "\n",
    "```\n",
    "model.docvecs['data_0']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STORAGE PROCEDURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_filename(s):\n",
    "    \"\"\"Take a string and return a valid filename constructed from the string.\n",
    "Uses a whitelist approach: any characters not present in valid_chars are\n",
    "removed. Also spaces are replaced with underscores.\n",
    " \n",
    "Note: this method may produce invalid filenames such as ``, `.` or `..`\n",
    "When I use this method I prepend a date string like '2009_01_15_19_46_32_'\n",
    "and append a file extension like '.txt', so I avoid the potential of using\n",
    "an invalid filename.\n",
    " \n",
    "from https://gist.github.com/seanh/93666\n",
    "\"\"\"\n",
    "    valid_chars = \"-_.() %s%s\" % (string.ascii_letters, string.digits)\n",
    "    filename = ''.join(c for c in s if c in valid_chars)\n",
    "    filename = filename.replace(' ','_') # I don't like spaces in filenames.\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(transformer, model, cleaner_name):\n",
    "    name = '%s-%s-%s' %(cleaner_name, transformer_name.lower(), model_name.lower())\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_results(res):\n",
    "    fname = get_filename_for_model_name(res['name'])\n",
    "    pickle.dump(res, open(fname, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_for_model_name(name):\n",
    "    name = format_filename(name)\n",
    "    name = '%s.model' %name\n",
    "    name = os.path.join('models', name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_grid_search_with_transformer(model):\n",
    "    return type(model) == sklearn.model_selection._search.GridSearchCV and len(model.estimator.steps) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vecs = np.zeros((len(X),100))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    prefix = 'data_%s' %i\n",
    "    vector = model.docvecs[prefix]\n",
    "    X_vecs[i] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    1\n",
      "0                    \n",
      "570306133677760513  0\n",
      "570301130888122368  1\n",
      "570301083672813571  0\n",
      "570301031407624196 -1\n",
      "570300817074462722 -1\n"
     ]
    }
   ],
   "source": [
    "y = pd.read_csv(\"y.csv\", index_col=0, header=None)\n",
    "print(y.head())\n",
    "y=y.values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    [\n",
    "        'LogisticRegression-l2',\n",
    "        LogisticRegression(max_iter=2000),\n",
    "        {\n",
    "            'C': (0.001, 0.01, 0.1, 1, 10, 100, 1000),\n",
    "            'penalty': ['l2'],\n",
    "            'class_weight': ('balanced', None),\n",
    "            'solver': ('newton-cg', 'sag', 'lbfgs')\n",
    "        },\n",
    "    ],\n",
    "    [\n",
    "        'LinearSVC',\n",
    "        LinearSVC(),\n",
    "        {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        'Perceptron',\n",
    "        Perceptron(),\n",
    "        {\n",
    "            'alpha': [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3],\n",
    "            'n_iter': [5, 10, 15, 20, 50],\n",
    "            'penalty': [None, 'l2', 'l1', 'elasticnet']\n",
    "        }\n",
    "    ],\n",
    "#     [\n",
    "#         'MultinomialNB',\n",
    "#         MultinomialNB(),\n",
    "#         {\n",
    "#             'alpha': [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "#         }\n",
    "#     ],\n",
    "    [\n",
    "        'DecisionTreeClassifier',\n",
    "        DecisionTreeClassifier(),\n",
    "        {\n",
    "            'min_samples_split' : range(10,500,20),\n",
    "            'max_depth': range(1,20,2)\n",
    "        } \n",
    "    ],\n",
    "       [\n",
    "        'RandomForestClassifier',\n",
    "        RandomForestClassifier(),\n",
    "        {\n",
    "            'max_depth': [10, 50, 110, None],\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'n_estimators': [200, 500, 1000]\n",
    "        }\n",
    "    ]    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec with LogisticRegression-l2 (stemmer cleaner)...\n",
      "Skipping. Model exists at models\\stemmer-doc2vec-logisticregression-l2.model\n",
      "doc2vec with LinearSVC (stemmer cleaner)...\n",
      "Skipping. Model exists at models\\stemmer-doc2vec-linearsvc.model\n",
      "doc2vec with Perceptron (stemmer cleaner)...\n",
      "Skipping. Model exists at models\\stemmer-doc2vec-perceptron.model\n",
      "doc2vec with DecisionTreeClassifier (stemmer cleaner)...\n",
      "Skipping. Model exists at models\\stemmer-doc2vec-decisiontreeclassifier.model\n",
      "doc2vec with RandomForestClassifier (stemmer cleaner)...\n",
      "Skipping. Model exists at models\\stemmer-doc2vec-randomforestclassifier.model\n"
     ]
    }
   ],
   "source": [
    "for model_name, model, model_params in models:\n",
    "    transformer_name = 'doc2vec'\n",
    "    cleaner_name = 'stemmer'\n",
    "    print('%s with %s (%s cleaner)...' %(transformer_name, model_name, cleaner_name))\n",
    "\n",
    "    name = get_model_name(model_name, transformer_name, cleaner_name)\n",
    "    fname = get_filename_for_model_name(name)\n",
    "\n",
    "    if not os.path.exists(fname):\n",
    "        K = 5\n",
    "\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                (model_name, model)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        new_params = {}\n",
    "\n",
    "        for k, v in model_params.items():\n",
    "            new_k = model_name + \"__\" + k\n",
    "            new_params[new_k] = v\n",
    "\n",
    "        parameters = {\n",
    "            **new_params\n",
    "        }\n",
    "        print(parameters)\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            pipeline,\n",
    "            parameters,\n",
    "            n_jobs=-1,\n",
    "            cv=K\n",
    "        )\n",
    "\n",
    "        grid.fit(X_vecs, y)\n",
    "        model = grid.best_estimator_\n",
    "        score = grid.best_score_\n",
    "\n",
    "        res = {\n",
    "            'name': name,\n",
    "            'transformer': transformer_name,\n",
    "            'transformer_name': transformer_name,\n",
    "            'model': model.get_params(),\n",
    "            'model_name': model_name,\n",
    "            'cleaner': cleaner_name,\n",
    "            'score': score,\n",
    "            }\n",
    "\n",
    "        dump_results(res)\n",
    "        print('Saved to %s' %fname)\n",
    "\n",
    "    else:\n",
    "        print('Skipping. Model exists at %s' %fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
